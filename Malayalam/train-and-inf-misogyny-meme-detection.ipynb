{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-05T13:02:18.676567Z",
     "iopub.status.busy": "2025-01-05T13:02:18.676322Z",
     "iopub.status.idle": "2025-01-05T13:02:21.813045Z",
     "shell.execute_reply": "2025-01-05T13:02:21.812315Z",
     "shell.execute_reply.started": "2025-01-05T13:02:18.676532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fe-misogyny-meme-detection/val_text_embeddings.pt\n",
      "/kaggle/input/fe-misogyny-meme-detection/test_text_embeddings.pt\n",
      "/kaggle/input/fe-misogyny-meme-detection/train_image_embeddings.pt\n",
      "/kaggle/input/fe-misogyny-meme-detection/val_image_embeddings.pt\n",
      "/kaggle/input/fe-misogyny-meme-detection/test_image_embeddings.pt\n",
      "/kaggle/input/fe-misogyny-meme-detection/train_text_embeddings.pt\n",
      "/kaggle/input/3-misogyny-meme-detection/3 Misogyny Meme Detection/Tamil/dev-20250101T183837Z-001/dev/dev.csv\n",
      "/kaggle/input/3-misogyny-meme-detection/3 Misogyny Meme Detection/Tamil/train-20250101T183816Z-001/train/train.csv\n",
      "/kaggle/input/3-misogyny-meme-detection/3 Misogyny Meme Detection/Tamil/test-20250101T183840Z-001/test/test.csv\n",
      "/kaggle/input/3-misogyny-meme-detection/3 Misogyny Meme Detection/Malayalam/train-20250101T182940Z-001/train/train.csv\n",
      "/kaggle/input/3-misogyny-meme-detection/3 Misogyny Meme Detection/Malayalam/dev-20250101T182941Z-001/dev/dev.csv\n",
      "/kaggle/input/3-misogyny-meme-detection/3 Misogyny Meme Detection/Malayalam/test-20250101T182944Z-001/test/test.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        if (filename.endswith(\".csv\")) or (filename.endswith(\".pt\")):\n",
    "            print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:02:21.814823Z",
     "iopub.status.busy": "2025-01-05T13:02:21.814447Z",
     "iopub.status.idle": "2025-01-05T13:02:21.846855Z",
     "shell.execute_reply": "2025-01-05T13:02:21.846196Z",
     "shell.execute_reply.started": "2025-01-05T13:02:21.814800Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"/kaggle/input/3-misogyny-meme-detection/3 Misogyny Meme Detection/Tamil/train-20250101T183816Z-001/train/train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:02:21.848194Z",
     "iopub.status.busy": "2025-01-05T13:02:21.847999Z",
     "iopub.status.idle": "2025-01-05T13:02:21.859555Z",
     "shell.execute_reply": "2025-01-05T13:02:21.858768Z",
     "shell.execute_reply.started": "2025-01-05T13:02:21.848177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_df=pd.read_csv(\"/kaggle/input/3-misogyny-meme-detection/3 Misogyny Meme Detection/Tamil/dev-20250101T183837Z-001/dev/dev.csv\")\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:02:21.860724Z",
     "iopub.status.busy": "2025-01-05T13:02:21.860409Z",
     "iopub.status.idle": "2025-01-05T13:02:21.873023Z",
     "shell.execute_reply": "2025-01-05T13:02:21.872366Z",
     "shell.execute_reply.started": "2025-01-05T13:02:21.860693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df=pd.read_csv(\"/kaggle/input/3-misogyny-meme-detection/3 Misogyny Meme Detection/Tamil/test-20250101T183840Z-001/test/test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:02:21.874148Z",
     "iopub.status.busy": "2025-01-05T13:02:21.873833Z",
     "iopub.status.idle": "2025-01-05T13:02:36.625233Z",
     "shell.execute_reply": "2025-01-05T13:02:36.624318Z",
     "shell.execute_reply.started": "2025-01-05T13:02:21.874119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from transformers import AutoModel, AutoTokenizer, AutoProcessor\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:02:36.627801Z",
     "iopub.status.busy": "2025-01-05T13:02:36.627359Z",
     "iopub.status.idle": "2025-01-05T13:02:36.631120Z",
     "shell.execute_reply": "2025-01-05T13:02:36.630338Z",
     "shell.execute_reply.started": "2025-01-05T13:02:36.627778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "TRAIN_IMAGE_FOLDER = \"/kaggle/input/3-misogyny-meme-detection/3 Misogyny Meme Detection/Tamil/train-20250101T183816Z-001/train\"\n",
    "VAL_IMAGE_FOLDER = \"/kaggle/input/3-misogyny-meme-detection/3 Misogyny Meme Detection/Tamil/dev-20250101T183837Z-001/dev\"\n",
    "TEST_IMAGE_FOLDER = \"/kaggle/input/3-misogyny-meme-detection/3 Misogyny Meme Detection/Tamil/test-20250101T183840Z-001/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:02:36.633110Z",
     "iopub.status.busy": "2025-01-05T13:02:36.632825Z",
     "iopub.status.idle": "2025-01-05T13:02:36.723146Z",
     "shell.execute_reply": "2025-01-05T13:02:36.722370Z",
     "shell.execute_reply.started": "2025-01-05T13:02:36.633089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IMAGE_MODEL_NAME = \"openai/clip-vit-base-patch32\"  # Example vision model\n",
    "TEXT_MODEL_NAME = \"Hate-speech-CNERG/tamil-codemixed-abusive-MuRIL\"  # Example Tamil BERT model\n",
    "BATCH_SIZE = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:02:36.724196Z",
     "iopub.status.busy": "2025-01-05T13:02:36.723982Z",
     "iopub.status.idle": "2025-01-05T13:02:36.737714Z",
     "shell.execute_reply": "2025-01-05T13:02:36.736951Z",
     "shell.execute_reply.started": "2025-01-05T13:02:36.724176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(embedding_path):\n",
    "    if os.path.exists(embedding_path):\n",
    "        print(f\"Loading embeddings from {embedding_path}\")\n",
    "        return torch.load(embedding_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Embeddings file not found at {embedding_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:02:36.738737Z",
     "iopub.status.busy": "2025-01-05T13:02:36.738513Z",
     "iopub.status.idle": "2025-01-05T13:02:36.974089Z",
     "shell.execute_reply": "2025-01-05T13:02:36.973393Z",
     "shell.execute_reply.started": "2025-01-05T13:02:36.738713Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from /kaggle/input/fe-misogyny-meme-detection/train_image_embeddings.pt\n",
      "Loading embeddings from /kaggle/input/fe-misogyny-meme-detection/val_image_embeddings.pt\n",
      "Loading embeddings from /kaggle/input/fe-misogyny-meme-detection/test_image_embeddings.pt\n",
      "Loading embeddings from /kaggle/input/fe-misogyny-meme-detection/train_text_embeddings.pt\n",
      "Loading embeddings from /kaggle/input/fe-misogyny-meme-detection/val_text_embeddings.pt\n",
      "Loading embeddings from /kaggle/input/fe-misogyny-meme-detection/test_text_embeddings.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f1b929861d2b>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(embedding_path)\n"
     ]
    }
   ],
   "source": [
    "# Load training, validation, and test embeddings\n",
    "train_image_embeddings = load_embeddings(\"/kaggle/input/fe-misogyny-meme-detection/train_image_embeddings.pt\")\n",
    "val_image_embeddings = load_embeddings(\"/kaggle/input/fe-misogyny-meme-detection/val_image_embeddings.pt\")\n",
    "test_image_embeddings = load_embeddings(\"/kaggle/input/fe-misogyny-meme-detection/test_image_embeddings.pt\")\n",
    "\n",
    "train_text_embeddings = load_embeddings(\"/kaggle/input/fe-misogyny-meme-detection/train_text_embeddings.pt\")\n",
    "val_text_embeddings = load_embeddings(\"/kaggle/input/fe-misogyny-meme-detection/val_text_embeddings.pt\")\n",
    "test_text_embeddings = load_embeddings(\"/kaggle/input/fe-misogyny-meme-detection/test_text_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:07:02.131793Z",
     "iopub.status.busy": "2025-01-05T13:07:02.131483Z",
     "iopub.status.idle": "2025-01-05T13:07:02.137037Z",
     "shell.execute_reply": "2025-01-05T13:07:02.136090Z",
     "shell.execute_reply.started": "2025-01-05T13:07:02.131770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combine_embeddings(image_embeddings, text_embeddings, df, has_labels=True):\n",
    "    combined_embeddings = []\n",
    "    labels = [] if has_labels else None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        image_id = row[\"image_id\"]\n",
    "        if image_id in image_embeddings and image_id in text_embeddings:\n",
    "            # Squeeze to remove unnecessary dimensions\n",
    "            image_embedding = image_embeddings[image_id].squeeze()\n",
    "            text_embedding = text_embeddings[image_id].squeeze()\n",
    "\n",
    "            # Combine image and text embeddings\n",
    "            combined = torch.cat([image_embedding, text_embedding], dim=-1)\n",
    "            combined_embeddings.append(combined)\n",
    "\n",
    "            if has_labels:\n",
    "                labels.append(row[\"labels\"])\n",
    "\n",
    "    if has_labels:\n",
    "        return torch.stack(combined_embeddings), torch.tensor(labels)\n",
    "    else:\n",
    "        return torch.stack(combined_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:07:08.265493Z",
     "iopub.status.busy": "2025-01-05T13:07:08.265110Z",
     "iopub.status.idle": "2025-01-05T13:07:08.350252Z",
     "shell.execute_reply": "2025-01-05T13:07:08.349389Z",
     "shell.execute_reply.started": "2025-01-05T13:07:08.265448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([640, 1280]), Labels: torch.Size([640])\n",
      "Validation data shape: torch.Size([160, 1280]), Labels: torch.Size([160])\n",
      "Test data shape: torch.Size([200, 1280])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = combine_embeddings(train_image_embeddings, train_text_embeddings, train_df)\n",
    "X_val, y_val = combine_embeddings(val_image_embeddings, val_text_embeddings, val_df)\n",
    "X_test = combine_embeddings(test_image_embeddings, test_text_embeddings, test_df, has_labels=False)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, Labels: {y_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}, Labels: {y_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:07:12.959760Z",
     "iopub.status.busy": "2025-01-05T13:07:12.959422Z",
     "iopub.status.idle": "2025-01-05T13:07:12.963384Z",
     "shell.execute_reply": "2025-01-05T13:07:12.962575Z",
     "shell.execute_reply.started": "2025-01-05T13:07:12.959733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:14:29.086279Z",
     "iopub.status.busy": "2025-01-05T13:14:29.086039Z",
     "iopub.status.idle": "2025-01-05T13:14:29.091034Z",
     "shell.execute_reply": "2025-01-05T13:14:29.090351Z",
     "shell.execute_reply.started": "2025-01-05T13:14:29.086257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[1], output_size),\n",
    "            nn.Sigmoid(),  # Use Sigmoid for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:07:15.937710Z",
     "iopub.status.busy": "2025-01-05T13:07:15.937408Z",
     "iopub.status.idle": "2025-01-05T13:07:15.941629Z",
     "shell.execute_reply": "2025-01-05T13:07:15.940928Z",
     "shell.execute_reply.started": "2025-01-05T13:07:15.937688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [786, 512]\n",
    "output_size = 1\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:14:33.823935Z",
     "iopub.status.busy": "2025-01-05T13:14:33.823595Z",
     "iopub.status.idle": "2025-01-05T13:14:33.828443Z",
     "shell.execute_reply": "2025-01-05T13:14:33.827799Z",
     "shell.execute_reply.started": "2025-01-05T13:14:33.823875Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare datasets and loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:14:35.514175Z",
     "iopub.status.busy": "2025-01-05T13:14:35.513834Z",
     "iopub.status.idle": "2025-01-05T13:14:35.531126Z",
     "shell.execute_reply": "2025-01-05T13:14:35.530282Z",
     "shell.execute_reply.started": "2025-01-05T13:14:35.514151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = MLPClassifier(input_size, hidden_sizes, output_size).to(device)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:14:41.331946Z",
     "iopub.status.busy": "2025-01-05T13:14:41.331588Z",
     "iopub.status.idle": "2025-01-05T13:14:41.340712Z",
     "shell.execute_reply": "2025-01-05T13:14:41.339907Z",
     "shell.execute_reply.started": "2025-01-05T13:14:41.331907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def train_and_save_best_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, save_dir):\n",
    "    best_f1 = -float('inf')  # Initialize to a very low value to track the best model\n",
    "    best_model_path = None  # Path to save the best model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        all_train_preds, all_train_labels = [], []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = model(inputs).squeeze()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and labels for metrics\n",
    "            preds = (outputs > 0.5).int()\n",
    "            all_train_preds.extend(preds.tolist())\n",
    "            all_train_labels.extend(labels.tolist())\n",
    "\n",
    "        # Calculate training metrics\n",
    "        train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
    "        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(\n",
    "            all_train_labels, all_train_preds, average=\"macro\"\n",
    "        )\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_val_preds, all_val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "                outputs = model(inputs).squeeze()\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Collect predictions and labels for metrics\n",
    "                preds = (outputs > 0.5).int()\n",
    "                all_val_preds.extend(preds.tolist())\n",
    "                all_val_labels.extend(labels.tolist())\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
    "            all_val_labels, all_val_preds, average=\"macro\"\n",
    "        )\n",
    "\n",
    "        # Print metrics for the current epoch\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}: \"\n",
    "            f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "            f\"Train Acc: {train_accuracy:.4f}, Prec: {train_precision:.4f}, Rec: {train_recall:.4f}, F1: {train_f1:.4f} | \"\n",
    "            f\"Val Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "            f\"Val Acc: {val_accuracy:.4f}, Prec: {val_precision:.4f}, Rec: {val_recall:.4f}, F1: {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save the model if it has the best F1 score on validation\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_path = f\"{save_dir}/best_model_epoch_{epoch + 1}_f1_{val_f1:.4f}.pth\"\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Best model saved with F1: {val_f1:.4f} at epoch {epoch + 1}\")\n",
    "\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:16:05.469582Z",
     "iopub.status.busy": "2025-01-05T13:16:05.469285Z",
     "iopub.status.idle": "2025-01-05T13:16:07.327331Z",
     "shell.execute_reply": "2025-01-05T13:16:07.326584Z",
     "shell.execute_reply.started": "2025-01-05T13:16:05.469560Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss: 0.2611, Train Acc: 0.8938, Prec: 0.8924, Rec: 0.8860, F1: 0.8889 | Val Loss: 0.3448, Val Acc: 0.8625, Prec: 0.8986, Rec: 0.8282, F1: 0.8448\n",
      "Best model saved with F1: 0.8448 at epoch 1\n",
      "Epoch 2/10: Train Loss: 0.2344, Train Acc: 0.9094, Prec: 0.9132, Rec: 0.8985, F1: 0.9044 | Val Loss: 0.2611, Val Acc: 0.8938, Prec: 0.9016, Rec: 0.8762, F1: 0.8855\n",
      "Best model saved with F1: 0.8855 at epoch 2\n",
      "Epoch 3/10: Train Loss: 0.2001, Train Acc: 0.9219, Prec: 0.9235, Rec: 0.9140, F1: 0.9181 | Val Loss: 0.2870, Val Acc: 0.8812, Prec: 0.8741, Rec: 0.8798, F1: 0.8766\n",
      "Epoch 4/10: Train Loss: 0.2016, Train Acc: 0.9266, Prec: 0.9261, Rec: 0.9210, F1: 0.9234 | Val Loss: 0.3050, Val Acc: 0.8625, Prec: 0.8730, Rec: 0.8393, F1: 0.8501\n",
      "Epoch 5/10: Train Loss: 0.2011, Train Acc: 0.9328, Prec: 0.9340, Rec: 0.9263, F1: 0.9297 | Val Loss: 0.2697, Val Acc: 0.9125, Prec: 0.9084, Rec: 0.9084, F1: 0.9084\n",
      "Best model saved with F1: 0.9084 at epoch 5\n",
      "Epoch 6/10: Train Loss: 0.1524, Train Acc: 0.9484, Prec: 0.9484, Rec: 0.9443, F1: 0.9463 | Val Loss: 0.3852, Val Acc: 0.9062, Prec: 0.8999, Rec: 0.9060, F1: 0.9026\n",
      "Epoch 7/10: Train Loss: 0.1312, Train Acc: 0.9469, Prec: 0.9465, Rec: 0.9430, F1: 0.9447 | Val Loss: 0.5230, Val Acc: 0.8250, Prec: 0.8264, Rec: 0.8418, F1: 0.8232\n",
      "Epoch 8/10: Train Loss: 0.1550, Train Acc: 0.9422, Prec: 0.9413, Rec: 0.9385, F1: 0.9398 | Val Loss: 0.3965, Val Acc: 0.9000, Prec: 0.8953, Rec: 0.8953, F1: 0.8953\n",
      "Epoch 9/10: Train Loss: 0.1215, Train Acc: 0.9547, Prec: 0.9563, Rec: 0.9496, F1: 0.9526 | Val Loss: 0.4503, Val Acc: 0.8875, Prec: 0.8804, Rec: 0.8933, F1: 0.8844\n",
      "Epoch 10/10: Train Loss: 0.1147, Train Acc: 0.9500, Prec: 0.9504, Rec: 0.9456, F1: 0.9478 | Val Loss: 0.3637, Val Acc: 0.8750, Prec: 0.8685, Rec: 0.8830, F1: 0.8721\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_dir = \"./saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "best_model_path = train_and_save_best_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    save_dir=save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:24:50.481918Z",
     "iopub.status.busy": "2025-01-05T13:24:50.481555Z",
     "iopub.status.idle": "2025-01-05T13:24:50.488350Z",
     "shell.execute_reply": "2025-01-05T13:24:50.487381Z",
     "shell.execute_reply.started": "2025-01-05T13:24:50.481876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_and_generate_submission(test_loader, best_model_path, submission_file_path):\n",
    "    # Load the best model with weights_only=True to avoid security warnings\n",
    "    model = MLPClassifier(input_size, hidden_sizes, output_size).to(device)\n",
    "    model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    test_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            # Ensure inputs are converted to a tensor and stacked into a batch if necessary\n",
    "            if isinstance(inputs, list):\n",
    "                # Convert each item to tensor using .detach() to avoid the user warning\n",
    "                inputs = [i.clone().detach().to(device) if isinstance(i, torch.Tensor) else torch.tensor(i).to(device) for i in inputs]\n",
    "                inputs = torch.stack(inputs)  # Stack them into a batch tensor\n",
    "            else:\n",
    "                inputs = inputs.to(device)  # If inputs is already a tensor, move it to device\n",
    "\n",
    "            outputs = model(inputs).squeeze()\n",
    "\n",
    "            # Predict binary labels\n",
    "            preds = (outputs > 0.5).int()\n",
    "            test_predictions.extend(preds.tolist())\n",
    "\n",
    "    # Prepare the submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': [i for i in test_df['image_id']],\n",
    "        'predictions': test_predictions\n",
    "    })\n",
    "\n",
    "    # Save the predictions to a CSV file\n",
    "    submission_df.to_csv(submission_file_path, index=False)\n",
    "    print(f\"Submission file saved to {submission_file_path}\")\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:24:55.451361Z",
     "iopub.status.busy": "2025-01-05T13:24:55.451084Z",
     "iopub.status.idle": "2025-01-05T13:24:55.488767Z",
     "shell.execute_reply": "2025-01-05T13:24:55.487953Z",
     "shell.execute_reply.started": "2025-01-05T13:24:55.451341Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "submission_file_path = \"submission.csv\"\n",
    "submission_df = predict_and_generate_submission(test_loader=test_loader, best_model_path=best_model_path, submission_file_path=submission_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:24:56.989284Z",
     "iopub.status.busy": "2025-01-05T13:24:56.989004Z",
     "iopub.status.idle": "2025-01-05T13:24:56.996813Z",
     "shell.execute_reply": "2025-01-05T13:24:56.995935Z",
     "shell.execute_reply.started": "2025-01-05T13:24:56.989264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6409484,
     "sourceId": 10350563,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 216201612,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
