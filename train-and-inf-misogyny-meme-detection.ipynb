{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2c449",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:16.648095Z",
     "iopub.status.busy": "2025-01-05T13:26:16.647858Z",
     "iopub.status.idle": "2025-01-05T13:26:21.067200Z",
     "shell.execute_reply": "2025-01-05T13:26:21.066251Z"
    },
    "papermill": {
     "duration": 4.426249,
     "end_time": "2025-01-05T13:26:21.068914",
     "exception": false,
     "start_time": "2025-01-05T13:26:16.642665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('.'):\n",
    "    for filename in filenames:\n",
    "        if (filename.endswith(\".csv\")) or (filename.endswith(\".pt\")):\n",
    "            print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ff6d2",
   "metadata": {},
   "source": [
    "Due to data sharing policy of the original dataset owners, some outputs are ommitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f05f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:21.079246Z",
     "iopub.status.busy": "2025-01-05T13:26:21.078888Z",
     "iopub.status.idle": "2025-01-05T13:26:21.118212Z",
     "shell.execute_reply": "2025-01-05T13:26:21.117517Z"
    },
    "papermill": {
     "duration": 0.045279,
     "end_time": "2025-01-05T13:26:21.119480",
     "exception": false,
     "start_time": "2025-01-05T13:26:21.074201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(\"dataset/Tamil/train-20250127T110108Z-001/train/train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6902b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:21.128676Z",
     "iopub.status.busy": "2025-01-05T13:26:21.128433Z",
     "iopub.status.idle": "2025-01-05T13:26:21.140259Z",
     "shell.execute_reply": "2025-01-05T13:26:21.139444Z"
    },
    "papermill": {
     "duration": 0.017744,
     "end_time": "2025-01-05T13:26:21.141477",
     "exception": false,
     "start_time": "2025-01-05T13:26:21.123733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_df=pd.read_csv(\"dataset/Tamil/dev-20250127T110118Z-001/dev/dev.csv\")\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a15126",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:21.150790Z",
     "iopub.status.busy": "2025-01-05T13:26:21.150582Z",
     "iopub.status.idle": "2025-01-05T13:26:21.161797Z",
     "shell.execute_reply": "2025-01-05T13:26:21.161004Z"
    },
    "papermill": {
     "duration": 0.017145,
     "end_time": "2025-01-05T13:26:21.163009",
     "exception": false,
     "start_time": "2025-01-05T13:26:21.145864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df=pd.read_csv(\"dataset/Tamil/test-20250127T110123Z-001/test/test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7b065a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:21.172879Z",
     "iopub.status.busy": "2025-01-05T13:26:21.172684Z",
     "iopub.status.idle": "2025-01-05T13:26:36.730957Z",
     "shell.execute_reply": "2025-01-05T13:26:36.730237Z"
    },
    "papermill": {
     "duration": 15.564715,
     "end_time": "2025-01-05T13:26:36.732474",
     "exception": false,
     "start_time": "2025-01-05T13:26:21.167759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from transformers import AutoModel, AutoTokenizer, AutoProcessor\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982aa532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:36.742976Z",
     "iopub.status.busy": "2025-01-05T13:26:36.742523Z",
     "iopub.status.idle": "2025-01-05T13:26:36.745847Z",
     "shell.execute_reply": "2025-01-05T13:26:36.745234Z"
    },
    "papermill": {
     "duration": 0.009684,
     "end_time": "2025-01-05T13:26:36.747104",
     "exception": false,
     "start_time": "2025-01-05T13:26:36.737420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "TRAIN_IMAGE_FOLDER = \"dataset/Tamil/train-20250127T110108Z-001/train/\"\n",
    "VAL_IMAGE_FOLDER = \"dataset/Tamil/dev-20250127T110118Z-001/dev/\"\n",
    "TEST_IMAGE_FOLDER = \"dataset/Tamil/test-20250127T110123Z-001/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c363fbb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:36.756725Z",
     "iopub.status.busy": "2025-01-05T13:26:36.756521Z",
     "iopub.status.idle": "2025-01-05T13:26:36.840964Z",
     "shell.execute_reply": "2025-01-05T13:26:36.840288Z"
    },
    "papermill": {
     "duration": 0.090619,
     "end_time": "2025-01-05T13:26:36.842229",
     "exception": false,
     "start_time": "2025-01-05T13:26:36.751610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IMAGE_MODEL_NAME = \"zer0int/CLIP-GmP-ViT-L-14\"  # Example vision model\n",
    "TEXT_MODEL_NAME = \"PosteriorAI/dravida_llama2_7b\"  # Example Tamil BERT model\n",
    "BATCH_SIZE = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6342b798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:36.852656Z",
     "iopub.status.busy": "2025-01-05T13:26:36.852435Z",
     "iopub.status.idle": "2025-01-05T13:26:36.856007Z",
     "shell.execute_reply": "2025-01-05T13:26:36.855369Z"
    },
    "papermill": {
     "duration": 0.010329,
     "end_time": "2025-01-05T13:26:36.857361",
     "exception": false,
     "start_time": "2025-01-05T13:26:36.847032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_embeddings(embedding_path):\n",
    "    if os.path.exists(embedding_path):\n",
    "        print(f\"Loading embeddings from {embedding_path}\")\n",
    "        return torch.load(embedding_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Embeddings file not found at {embedding_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd0ef3c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:36.867792Z",
     "iopub.status.busy": "2025-01-05T13:26:36.867592Z",
     "iopub.status.idle": "2025-01-05T13:26:37.054685Z",
     "shell.execute_reply": "2025-01-05T13:26:37.053737Z"
    },
    "papermill": {
     "duration": 0.193574,
     "end_time": "2025-01-05T13:26:37.056087",
     "exception": false,
     "start_time": "2025-01-05T13:26:36.862513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from train_image_embeddings.pt\n",
      "Loading embeddings from val_image_embeddings.pt\n",
      "Loading embeddings from test_image_embeddings.pt\n",
      "Loading embeddings from train_text_embeddings.pt\n",
      "Loading embeddings from val_text_embeddings.pt\n",
      "Loading embeddings from test_text_embeddings.pt\n"
     ]
    }
   ],
   "source": [
    "# Load training, validation, and test embeddings\n",
    "train_image_embeddings = load_embeddings(\"train_image_embeddings.pt\")\n",
    "val_image_embeddings = load_embeddings(\"val_image_embeddings.pt\")\n",
    "test_image_embeddings = load_embeddings(\"test_image_embeddings.pt\")\n",
    "\n",
    "train_text_embeddings = load_embeddings(\"train_text_embeddings.pt\")\n",
    "val_text_embeddings = load_embeddings(\"val_text_embeddings.pt\")\n",
    "test_text_embeddings = load_embeddings(\"test_text_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a05706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:37.066318Z",
     "iopub.status.busy": "2025-01-05T13:26:37.066072Z",
     "iopub.status.idle": "2025-01-05T13:26:37.070827Z",
     "shell.execute_reply": "2025-01-05T13:26:37.070198Z"
    },
    "papermill": {
     "duration": 0.010976,
     "end_time": "2025-01-05T13:26:37.071910",
     "exception": false,
     "start_time": "2025-01-05T13:26:37.060934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_embeddings(image_embeddings, text_embeddings, df, has_labels=True):\n",
    "    combined_embeddings = []\n",
    "    labels = [] if has_labels else None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        image_id = row[\"image_id\"]\n",
    "        if image_id in image_embeddings and image_id in text_embeddings:\n",
    "            # Squeeze to remove unnecessary dimensions\n",
    "            image_embedding = image_embeddings[image_id].squeeze()\n",
    "            text_embedding = text_embeddings[image_id].squeeze()\n",
    "\n",
    "            # Combine image and text embeddings\n",
    "            combined = torch.cat([image_embedding, text_embedding], dim=-1)\n",
    "            combined_embeddings.append(combined)\n",
    "\n",
    "            if has_labels:\n",
    "                labels.append(row[\"labels\"])\n",
    "\n",
    "    if has_labels:\n",
    "        return torch.stack(combined_embeddings), torch.tensor(labels)\n",
    "    else:\n",
    "        return torch.stack(combined_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd7632d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:37.081511Z",
     "iopub.status.busy": "2025-01-05T13:26:37.081286Z",
     "iopub.status.idle": "2025-01-05T13:26:37.202608Z",
     "shell.execute_reply": "2025-01-05T13:26:37.201795Z"
    },
    "papermill": {
     "duration": 0.127512,
     "end_time": "2025-01-05T13:26:37.203908",
     "exception": false,
     "start_time": "2025-01-05T13:26:37.076396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([416, 4864]), Labels: torch.Size([416])\n",
      "Validation data shape: torch.Size([15, 4864]), Labels: torch.Size([15])\n",
      "Test data shape: torch.Size([43, 4864])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = combine_embeddings(train_image_embeddings, train_text_embeddings, train_df)\n",
    "X_val, y_val = combine_embeddings(val_image_embeddings, val_text_embeddings, val_df)\n",
    "X_test = combine_embeddings(test_image_embeddings, test_text_embeddings, test_df, has_labels=False)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}, Labels: {y_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}, Labels: {y_val.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a30a87c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:37.214051Z",
     "iopub.status.busy": "2025-01-05T13:26:37.213845Z",
     "iopub.status.idle": "2025-01-05T13:26:37.216762Z",
     "shell.execute_reply": "2025-01-05T13:26:37.216145Z"
    },
    "papermill": {
     "duration": 0.00916,
     "end_time": "2025-01-05T13:26:37.217840",
     "exception": false,
     "start_time": "2025-01-05T13:26:37.208680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e54e2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:37.227766Z",
     "iopub.status.busy": "2025-01-05T13:26:37.227565Z",
     "iopub.status.idle": "2025-01-05T13:26:37.231453Z",
     "shell.execute_reply": "2025-01-05T13:26:37.230843Z"
    },
    "papermill": {
     "duration": 0.010113,
     "end_time": "2025-01-05T13:26:37.232563",
     "exception": false,
     "start_time": "2025-01-05T13:26:37.222450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "class MLPClassifier(nn.Module):\n",
    "    def _init_(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLPClassifier, self)._init_()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_sizes[1], output_size),\n",
    "            nn.Sigmoid(),  # Use Sigmoid for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33935b86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:37.242440Z",
     "iopub.status.busy": "2025-01-05T13:26:37.242198Z",
     "iopub.status.idle": "2025-01-05T13:26:37.245229Z",
     "shell.execute_reply": "2025-01-05T13:26:37.244680Z"
    },
    "papermill": {
     "duration": 0.009318,
     "end_time": "2025-01-05T13:26:37.246461",
     "exception": false,
     "start_time": "2025-01-05T13:26:37.237143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [1024, 512]\n",
    "output_size = 1\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96e018e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:37.256138Z",
     "iopub.status.busy": "2025-01-05T13:26:37.255942Z",
     "iopub.status.idle": "2025-01-05T13:26:37.259649Z",
     "shell.execute_reply": "2025-01-05T13:26:37.259051Z"
    },
    "papermill": {
     "duration": 0.009838,
     "end_time": "2025-01-05T13:26:37.260822",
     "exception": false,
     "start_time": "2025-01-05T13:26:37.250984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare datasets and loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d358a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:37.270693Z",
     "iopub.status.busy": "2025-01-05T13:26:37.270491Z",
     "iopub.status.idle": "2025-01-05T13:26:37.525723Z",
     "shell.execute_reply": "2025-01-05T13:26:37.525023Z"
    },
    "papermill": {
     "duration": 0.261772,
     "end_time": "2025-01-05T13:26:37.527209",
     "exception": false,
     "start_time": "2025-01-05T13:26:37.265437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = MLPClassifier(input_size, hidden_sizes, output_size).to(device)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ef17f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:37.537890Z",
     "iopub.status.busy": "2025-01-05T13:26:37.537657Z",
     "iopub.status.idle": "2025-01-05T13:26:37.545901Z",
     "shell.execute_reply": "2025-01-05T13:26:37.545286Z"
    },
    "papermill": {
     "duration": 0.014825,
     "end_time": "2025-01-05T13:26:37.547067",
     "exception": false,
     "start_time": "2025-01-05T13:26:37.532242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def train_and_save_best_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, save_dir):\n",
    "    best_f1 = -float('inf')  # Initialize to a very low value to track the best model\n",
    "    best_model_path = None  # Path to save the best model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        all_train_preds, all_train_labels = [], []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "            outputs = model(inputs).squeeze()\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and labels for metrics\n",
    "            preds = (outputs > 0.5).int()\n",
    "            all_train_preds.extend(preds.tolist())\n",
    "            all_train_labels.extend(labels.tolist())\n",
    "\n",
    "        # Calculate training metrics\n",
    "        train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n",
    "        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(\n",
    "            all_train_labels, all_train_preds, average=\"macro\"\n",
    "        )\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_val_preds, all_val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device).float()\n",
    "                outputs = model(inputs).squeeze()\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Collect predictions and labels for metrics\n",
    "                preds = (outputs > 0.5).int()\n",
    "                all_val_preds.extend(preds.tolist())\n",
    "                all_val_labels.extend(labels.tolist())\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
    "            all_val_labels, all_val_preds, average=\"macro\",\n",
    "        )\n",
    "\n",
    "        # Print metrics for the current epoch\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}: \"\n",
    "            f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "            f\"Train Acc: {train_accuracy:.4f}, Prec: {train_precision:.4f}, Rec: {train_recall:.4f}, F1: {train_f1:.4f} | \"\n",
    "            f\"Val Loss: {val_loss / len(val_loader):.4f}, \"\n",
    "            f\"Val Acc: {val_accuracy:.4f}, Prec: {val_precision:.4f}, Rec: {val_recall:.4f}, F1: {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save the model if it has the best F1 score on validation\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_path = f\"{save_dir}/best_model_epoch_{epoch + 1}_f1_{val_f1:.4f}.pth\"\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Best model saved with F1: {val_f1:.4f} at epoch {epoch + 1}\")\n",
    "\n",
    "    return best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3ccd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:37.556890Z",
     "iopub.status.busy": "2025-01-05T13:26:37.556682Z",
     "iopub.status.idle": "2025-01-05T13:26:40.028882Z",
     "shell.execute_reply": "2025-01-05T13:26:40.027951Z"
    },
    "papermill": {
     "duration": 2.478698,
     "end_time": "2025-01-05T13:26:40.030318",
     "exception": false,
     "start_time": "2025-01-05T13:26:37.551620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_dir = \"./saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "best_model_path = train_and_save_best_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    save_dir=save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02748de2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:40.041354Z",
     "iopub.status.busy": "2025-01-05T13:26:40.041097Z",
     "iopub.status.idle": "2025-01-05T13:26:40.046900Z",
     "shell.execute_reply": "2025-01-05T13:26:40.046280Z"
    },
    "papermill": {
     "duration": 0.01235,
     "end_time": "2025-01-05T13:26:40.048007",
     "exception": false,
     "start_time": "2025-01-05T13:26:40.035657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_and_generate_submission(test_loader, best_model_path, submission_file_path):\n",
    "    # Load the best model with weights_only=True to avoid security warnings\n",
    "    model = MLPClassifier(input_size, hidden_sizes, output_size).to(device)\n",
    "    model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    test_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            # Ensure inputs are converted to a tensor and stacked into a batch if necessary\n",
    "            if isinstance(inputs, list):\n",
    "                # Convert each item to tensor using .detach() to avoid the user warning\n",
    "                inputs = [i.clone().detach().to(device) if isinstance(i, torch.Tensor) else torch.tensor(i).to(device) for i in inputs]\n",
    "                inputs = torch.stack(inputs)  # Stack them into a batch tensor\n",
    "            else:\n",
    "                inputs = inputs.to(device)  # If inputs is already a tensor, move it to device\n",
    "\n",
    "            outputs = model(inputs).squeeze()\n",
    "\n",
    "            # Predict binary labels\n",
    "            preds = (outputs > 0.5).int()\n",
    "            test_predictions.extend(preds.tolist())\n",
    "\n",
    "    # Prepare the submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': [i for i in test_df['image_id']],\n",
    "        'predictions': test_predictions\n",
    "    })\n",
    "\n",
    "    # Save the predictions to a CSV file\n",
    "    submission_df.to_csv(submission_file_path, index=False)\n",
    "    print(f\"Submission file saved to {submission_file_path}\")\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3cc54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:40.058853Z",
     "iopub.status.busy": "2025-01-05T13:26:40.058654Z",
     "iopub.status.idle": "2025-01-05T13:26:40.099462Z",
     "shell.execute_reply": "2025-01-05T13:26:40.098771Z"
    },
    "papermill": {
     "duration": 0.047472,
     "end_time": "2025-01-05T13:26:40.100692",
     "exception": false,
     "start_time": "2025-01-05T13:26:40.053220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_file_path = \"submission.csv\"\n",
    "submission_df = predict_and_generate_submission(test_loader=test_loader, best_model_path=best_model_path, submission_file_path=submission_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f6a5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T13:26:40.111034Z",
     "iopub.status.busy": "2025-01-05T13:26:40.110810Z",
     "iopub.status.idle": "2025-01-05T13:26:40.117202Z",
     "shell.execute_reply": "2025-01-05T13:26:40.116354Z"
    },
    "papermill": {
     "duration": 0.012893,
     "end_time": "2025-01-05T13:26:40.118503",
     "exception": false,
     "start_time": "2025-01-05T13:26:40.105610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6409484,
     "sourceId": 10350563,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 216201612,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28.876518,
   "end_time": "2025-01-05T13:26:43.118174",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-05T13:26:14.241656",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
